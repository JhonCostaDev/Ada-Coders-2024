{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databriks\n",
    "<!--TODO: Inserir imagens-->\n",
    "\n",
    "Desenvolvido pelos mesmos criadores do Apache Spark, o Databriks é uma plataforma de análise de dados unificada para engenharia de dados, machine learning e ciência de dados colaborativa. Se organiza através de workspaces, que são ambientes de software como serviço (SaaS) utilizados para acessar suas funcionalidades e objetos, como notebooks, em pastas e recursos computacionais, como clusters.\n",
    "\n",
    "## Vantagens de utilizar Databriks?\n",
    "\n",
    "* __Workspaces colaborativo__: Usuários diferentes podem compartilhar seus notebooks ou até mesmo trabalhar em um de forma simultânea.\n",
    "\n",
    "* __Ciêntistas, Analistas e Engenheiros de Dados__: Permite que cientistas de dados manipulem dados e criem modelos para Machine Learning; que analistas usem tabelas estruturadas, arquivos semi-estruturados e se conectem a ferramenta de visualização nativamente e que **engenheiros de dados extraiam, transformem e carreguem dados em diferentes ambientes**.\n",
    "\n",
    "* __Pluraridade de Linguagens__: Similar ao Jupyter Notebook, no Databriks é possível utilizar diversas linguagens de programação simultâneamente como por exemplo Python, R, SQL e Scala. Podendo até mescla-las em um mesmo notebook.\n",
    "\n",
    "* __Multi-Cloud__: Pode se conectar facilmente a diferentes plataformas de computação em nuvem.\n",
    "\n",
    "## Versão Community\n",
    "\n",
    "Versão gratuita para que estudadntes e interessados na plataforma possam praticar sem custo embora com limitações.\n",
    "\n",
    "## Cluster\n",
    "\n",
    "<!--TODO: pesquisar definição-->\n",
    "\n",
    "## Notebook\n",
    "\n",
    "Um notebook é uma interface baseada na Web para um documento que contém código executável, visualizações e texto de narração. Composto por células capazes de executar códigos em diferentes linguagens, é em um notebook que somos capazes de manipular arquivos, criar funções e tabelas. Utiliza o mesmo conceito e extensão que o Jupyter Notebook.\n",
    "\n",
    "### Limitações da versão Community\n",
    "\n",
    "* O cluster será desligado automáticamente apoós duas horas de inatividade, e não pode ser reinicializado. Com isso, embora os arquivos subidos no DBFS e notebooks não sejam perdidos, metadados como tabelas criadas serão.\n",
    "\n",
    "* Conexão com serviços de armazenamento (mount), como o S3 e BLob Storage, utilizando chaves de segurança não são permitidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EXTRAÇÕES AUTOMATIZADAS DE DADOS ENTRE SISTEMAS\n",
    "\n",
    "Olá, seja bem-vindo a este módulo sobre extrações automatizadas de dados entre sistemas!\n",
    "\n",
    "Nos dias de hoje, as empresas lidam com grandes quantidades de dados em diferentes sistemas. Para tomar decisões informadas e eficientes, é crucial ter acesso a todos esses dados de forma integrada e atualizada. Aqui estão algumas razões pelas quais as extrações automatizadas de dados são necessárias:\n",
    "\n",
    "1. **Aumento da eficiência**: extrair manualmente dados de diferentes sistemas é um processo demorado e propenso a erros. Com extrações automatizadas, é possível obter dados rapidamente e com precisão, permitindo uma tomada de decisão mais rápida e eficiente;\n",
    "\n",
    "2. **Redução de erros**: as extrações automatizadas removem a possibilidade de erros humanos, como digitação incorreta ou omissão de informações;\n",
    "\n",
    "3. **Integração de dados**: as extrações automatizadas permitem integrar dados de diferentes sistemas, fornecendo uma visão mais completa e precisa do negócio; \n",
    "    \n",
    "4. **Tempo e recursos**: automatizar o processo de extração de dados economiza tempo e recursos, permitindo que os funcionários se concentrem em tarefas mais importantes.\n",
    "\n",
    "Em resumo, as extrações automatizadas de dados são fundamentais para obter dados precisos, integrados e atualizados, aumentando a eficiência e efetividade da tomada de decisão.\n",
    "\n",
    "\n",
    "\n",
    "## ARQUITETURA DE SOLUÇÕES DE EXTRAÇÃO DE DADOS\n",
    "<hr>\n",
    "\n",
    "A arquitetura de soluções de extração de dados pode envolver o uso de APIs (Application Programming Interfaces), webhooks, planilhas, etc. para obter dados de diferentes fontes de forma programática. Neste tipo de solução, a extração de dados é automatizada usando scripts escritos em alguma linguagem de programção (como por exemplo, Python). De forma simples, podemos definir um processo de extração de dados em 5 partes:\n",
    "\n",
    "1. **Fonte de dados**: a primeira etapa é identificar a fonte de dados desejada. Isso pode incluir bancos de dados, sistemas de gerenciamento de conteúdo (CMS), plataformas de e-commerce, entre outros;\n",
    "\n",
    "2. **API e Webhooks**: a próxima etapa é acessar sua fonte de dados. A API é um conjunto de regras que permite que você interaja com a fonte de dados de forma programática, enquanto que os webhooks possibitam o recebimento assíncrono dos seus dados. Algumas fontes de dados possuem APIs públicas, enquanto outras exigem autenticação ou pagamento por volume de dados consumido;\n",
    "\n",
    "3. **Requisição API**: uma vez que você tenha acesso à API, podemos enviar requisições para obter os dados desejados. Em Python, isso pode ser feito usando bibliotecas como **Requests ou httplib2**;\n",
    "\n",
    "4. **Tratamento de dados**: após a recepção dos dados, precisamos tratá-los antes de serem usados. Isso pode incluir a limpeza dos dados, a conversão para um formato desejado e sua posterior análise. Em Python, podemos utilizar bibliotecas como Pandas ou Numpy para realizar tais tarefas.\n",
    "\n",
    "5. **Armazenamento de dados**: por fim, os dados extraídos precisam ser armazenados em algum local para uso futuro. E, isso pode ser feito em bancos de dados locais ou na nuvem, como o Amazon S3 ou o Google Cloud Storage.\n",
    "\n",
    "É importante destacar que a soma destas partes permite a automatização da extração de dados, tornando-a mais eficiente e confiável. Além disso, a utilização de Python como linguagem de programação permite a facilidade de uso e a ampla disponibilidade de bibliotecas para tratamento e armazenamento de dados.\n",
    "\n",
    "\n",
    "\n",
    "## ETLs & ELTs\n",
    "\n",
    "ETL, ELT e pipeline de dados são conceitos importantes na gestão de dados. E, podemos defini-los da seguinte forma:\n",
    "\n",
    "**ETL (Extração, Transformação e Carga):** <hl>\n",
    "    \n",
    "ETL é uma sigla que se refere ao processo de extração de dados de várias fontes, as transformações utilizadas para torná-los compatíveis com o sistema de destino e ao seu carregamento no sistema de destino. Esse processo é importante para garantir que os dados sejam integrados corretamente em um sistema único e possam posteriormente serem utilizados para a tomada de decisão informada.\n",
    "\n",
    "**ELT (Extração, Carga e Transformação):** \n",
    "\n",
    "a ELT é semelhante ao ETL, mas a ordem dos passos é diferente. Em uma ELT, os dados são extraídos de fontes externas, carregados no sistema de destino e, em seguida, transformados para serem compatíveis com o sistema de destino. A ELT é uma abordagem popular quando se tem um hardware poderoso disponível para transformação de dados no lado do sistema de destino.\n",
    "\n",
    "**Pipeline de dados com extração ativa ou passiva**: \n",
    "\n",
    "Pipeline de dados se referem ao processo de fluxo contínuo de dados de uma fonte para um sistema de destino. Elas podem ser divididas em duas categorias:\n",
    "\n",
    "* **Pipelines de extração ativa**: são aquelas em que os dados são extraídos da fonte em intervalos regulares de tempo; \n",
    "\n",
    "* **Pipelines de extração passiva**: são aquelas em que os dados são enviados assincronamente pela sistema originador da informação;\n",
    "\n",
    "Em suma, ETL, ELT e pipeline de dados são conceitos importantes para a gestão de dados que ajudam a garantir que os dados sejam integrados corretamente em um único sistema e possam ser usados para tomar decisões informadas. A escolha entre ETL e ELT depende de fatores como a quantidade de dados e o poder de processamento disponível, enquanto que a escolha entre extração ativa ou passiva irá depender de outros fatores como a volumetria dos dados e o tempo de latência requisitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>290.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>290.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11217</th>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>677352.0</td>\n",
       "      <td>678234.0</td>\n",
       "      <td>671000.0</td>\n",
       "      <td>671750.0</td>\n",
       "      <td>671750.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11218</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>673038.0</td>\n",
       "      <td>678827.0</td>\n",
       "      <td>671436.0</td>\n",
       "      <td>677731.0</td>\n",
       "      <td>677731.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11219</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>680703.0</td>\n",
       "      <td>691164.0</td>\n",
       "      <td>679124.0</td>\n",
       "      <td>685250.0</td>\n",
       "      <td>685250.0</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11220</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>687500.0</td>\n",
       "      <td>688902.0</td>\n",
       "      <td>682214.0</td>\n",
       "      <td>686859.0</td>\n",
       "      <td>686859.0</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>692979.0</td>\n",
       "      <td>693775.0</td>\n",
       "      <td>686000.0</td>\n",
       "      <td>687840.0</td>\n",
       "      <td>687840.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11222 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date      Open      High       Low     Close  Adj Close  Volume\n",
       "0      1980-03-17     290.0     310.0     290.0     290.0      290.0   10000\n",
       "1      1980-03-18     290.0     290.0     290.0     290.0      290.0       0\n",
       "2      1980-03-19     290.0     310.0     290.0     290.0      290.0   20000\n",
       "3      1980-03-20     290.0     290.0     290.0     290.0      290.0       0\n",
       "4      1980-03-21     290.0     290.0     290.0     290.0      290.0       0\n",
       "...           ...       ...       ...       ...       ...        ...     ...\n",
       "11217  2024-09-13  677352.0  678234.0  671000.0  671750.0   671750.0    1900\n",
       "11218  2024-09-16  673038.0  678827.0  671436.0  677731.0   677731.0    1900\n",
       "11219  2024-09-17  680703.0  691164.0  679124.0  685250.0   685250.0    1300\n",
       "11220  2024-09-18  687500.0  688902.0  682214.0  686859.0   686859.0    1200\n",
       "11221  2024-09-19  692979.0  693775.0  686000.0  687840.0   687840.0    1500\n",
       "\n",
       "[11222 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de ETL em Python\n",
    "\n",
    "# importar biblioteca\n",
    "import pandas as pd\n",
    "\n",
    "# Extrair os dados de um arquivo csv\n",
    "df = pd.read_csv('/var/home/jhon/Documents/GitHub/Ada-Coders-2024/data_engineering/tecnicas_programação/aula_3/archive/BRK-A.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando os nomes das colunas para uma lista\n",
    "atual = df.keys().tolist()\n",
    "# Criando uma lista com os novos nomes das colunas\n",
    "nova = ['Data', 'Abertura', 'Alta', 'Baixa', 'Fechamento', 'Ajuste Fechamento', 'Volume']\n",
    "# Adicionando tudo a um dicionário com a função zip\n",
    "dicionario = dict(zip(atual, nova))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Abertura</th>\n",
       "      <th>Alta</th>\n",
       "      <th>Baixa</th>\n",
       "      <th>Fechamento</th>\n",
       "      <th>Ajuste Fechamento</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-03-17</td>\n",
       "      <td>290.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>290.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11217</th>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>677352.0</td>\n",
       "      <td>678234.0</td>\n",
       "      <td>671000.0</td>\n",
       "      <td>671750.0</td>\n",
       "      <td>671750.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11218</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>673038.0</td>\n",
       "      <td>678827.0</td>\n",
       "      <td>671436.0</td>\n",
       "      <td>677731.0</td>\n",
       "      <td>677731.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11219</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>680703.0</td>\n",
       "      <td>691164.0</td>\n",
       "      <td>679124.0</td>\n",
       "      <td>685250.0</td>\n",
       "      <td>685250.0</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11220</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>687500.0</td>\n",
       "      <td>688902.0</td>\n",
       "      <td>682214.0</td>\n",
       "      <td>686859.0</td>\n",
       "      <td>686859.0</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>692979.0</td>\n",
       "      <td>693775.0</td>\n",
       "      <td>686000.0</td>\n",
       "      <td>687840.0</td>\n",
       "      <td>687840.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11222 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data  Abertura      Alta     Baixa  Fechamento  \\\n",
       "0      1980-03-17     290.0     310.0     290.0       290.0   \n",
       "1      1980-03-18     290.0     290.0     290.0       290.0   \n",
       "2      1980-03-19     290.0     310.0     290.0       290.0   \n",
       "3      1980-03-20     290.0     290.0     290.0       290.0   \n",
       "4      1980-03-21     290.0     290.0     290.0       290.0   \n",
       "...           ...       ...       ...       ...         ...   \n",
       "11217  2024-09-13  677352.0  678234.0  671000.0    671750.0   \n",
       "11218  2024-09-16  673038.0  678827.0  671436.0    677731.0   \n",
       "11219  2024-09-17  680703.0  691164.0  679124.0    685250.0   \n",
       "11220  2024-09-18  687500.0  688902.0  682214.0    686859.0   \n",
       "11221  2024-09-19  692979.0  693775.0  686000.0    687840.0   \n",
       "\n",
       "       Ajuste Fechamento  Volume  \n",
       "0                  290.0   10000  \n",
       "1                  290.0       0  \n",
       "2                  290.0   20000  \n",
       "3                  290.0       0  \n",
       "4                  290.0       0  \n",
       "...                  ...     ...  \n",
       "11217           671750.0    1900  \n",
       "11218           677731.0    1900  \n",
       "11219           685250.0    1300  \n",
       "11220           686859.0    1200  \n",
       "11221           687840.0    1500  \n",
       "\n",
       "[11222 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar os dados lidos para a forma desejada\n",
    "\n",
    "# Renomeando as colunas\n",
    "df.rename(columns= dicionario, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados transformados em um novo csv\n",
    "\n",
    "# Salvando o DataFrame em um arquivo CSV\n",
    "df.to_csv('BRK-Rename.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dados_origem.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extrair dados de um arquivo CSV\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdados_origem.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m arquivo:\n\u001b[1;32m      7\u001b[0m     dados \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(csv\u001b[38;5;241m.\u001b[39mreader(arquivo))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Transformar os dados lidos para a forma desejada\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Ada-Coders-2024/data_engineering/orientacao_objetos/projetoPOO_ada/.venv/lib64/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dados_origem.csv'"
     ]
    }
   ],
   "source": [
    "# Importar biblioteca para ler arquivos CSV\n",
    "import csv\n",
    "\n",
    "\n",
    "# Extrair dados de um arquivo CSV\n",
    "with open(\"dados_origem.csv\", \"r\") as arquivo:\n",
    "    dados = list(csv.reader(arquivo))\n",
    "\n",
    "\n",
    "# Transformar os dados lidos para a forma desejada\n",
    "dados_transformados = []\n",
    "for linha in dados:\n",
    "    nome, idade = linha[0], int(linha[1])\n",
    "    dados_transformados.append({\"nome\": nome, \"idade\": idade})\n",
    "\n",
    "\n",
    "# Carregar os dados transformados em um banco de dados\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "conexao = sqlite3.connect(\"dados_destino.db\")\n",
    "cursor = conexao.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS pessoas (nome TEXT, idade INTEGER)\")\n",
    "for pessoa in dados_transformados:\n",
    "    cursor.execute(\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", (pessoa[\"nome\"], pessoa[\"idade\"]))\n",
    "\n",
    "\n",
    "conexao.commit()\n",
    "conexao.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de um ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar biblioteca para ler arquivos CSV\n",
    "import csv\n",
    "\n",
    "\n",
    "# Importar biblioteca para conexão com banco de dados\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Extrair dados de um arquivo CSV e carregá-los diretamente no banco de dados\n",
    "conexao = sqlite3.connect(\"dados_destino.db\")\n",
    "cursor = conexao.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS pessoas (nome TEXT, idade INTEGER)\")\n",
    "with open(\"dados_origem.csv\", \"r\") as arquivo:\n",
    "    dados = list(csv.reader(arquivo))\n",
    "    for linha in dados:\n",
    "        cursor.execute(\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", linha)\n",
    "\n",
    "\n",
    "conexao.commit()\n",
    "conexao.close()\n",
    "\n",
    "\n",
    "# Transformar os dados carregados no banco de dados\n",
    "conexao = sqlite3.connect(\"dados_destino.db\")\n",
    "cursor = conexao.cursor()\n",
    "\n",
    "\n",
    "cursor.execute(\"UPDATE pessoas SET idade = idade + 1\")\n",
    "\n",
    "\n",
    "conexao.commit()\n",
    "conexao.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximo tópico a estudar API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
