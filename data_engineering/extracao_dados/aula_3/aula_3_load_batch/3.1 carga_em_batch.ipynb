{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b8b956-dc92-4cb6-a366-bb534a15736f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "\n",
    "## Módulo: ED-NA-001 - Extração de Dados I\n",
    "<br>\n",
    "\n",
    "## Aula 3 - Parte 1\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f538c38-75d4-48f5-b85a-e2fff4101a66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Programação da Aula 2:\n",
    "\n",
    "> #### 1. **O que é uma carga em batch ou processamento em lotes e a diferença com um streaming de dados**;\n",
    "> #### 2. **Exemplos de carga em batch**;\n",
    "> #### 3. **O que é um Webhook**;\n",
    "> #### 4. **Implementando um Webhook**;\n",
    "> #### 5. **Exercício 1: Desenvolver um pipeline de dados com processamento em lotes;**\n",
    "> #### 6. **Exercício 2: Implementar um Webhook com uma API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "071d1660-293f-41a2-81cd-70a821788f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##O que é uma carga em batch ou processamento em lotes? Qual é a diferença com um streaming de dados?\n",
    "\n",
    "#### O processamento em lotes envolve a análise de dados em intervalos fixos de tempo, o que resulta em uma latência maior, sendo adequado para análises retrospectivas, enquanto o processamento de streaming processa dados em tempo real à medida que são gerados, oferecendo baixa latência e sendo ideal para análises em tempo real e tomada de decisões imediatas com base nos dados recebidos.\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## Exemplo de carga em batch\n",
    "<img src=\"./processamento_batch.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Exemplo de streaming\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./processamento_streaming.png\"  width=\"70%\" height=\"40%\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a64be81-f60d-4f78-9a36-a559ee3558c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exemplo de carga em batch\n",
    "### Implementar um processamento a lote que a cada 5 segundos percorre um diretório no DBFS buscando novos arquivos e carrega os novos resultados em um arquivo final de formato parquet. Após a carga, efetua uma transformação dos dados e armazena os resultados em outro arquivo parquet. \n",
    "### Os arquivos brutos de formato parquet são da seguinte base de dados do Kaggle relacionada a voos atrasados e cancelados: \n",
    "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebd969b3-7989-4eb4-b83c-805678156e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lista os arquivos brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/tables/dados_brutos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce8702bd-54e0-428d-82a5-377e70903c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/dados_brutos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee986beb-25d6-4953-a5bc-00ee8587c02d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A parit da listagem, é possível pegar os nomes dos arquivos brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7cfa9c-0aaa-4c1e-ade6-e0c7b45eda12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/dados_brutos/\")[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be298f8-d47b-4d19-b8a9-eb24af39a6f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#imprime o nome dos arquivos brutos\n",
    "for arquivo in dbutils.fs.ls(\"/FileStore/tables/dados_brutos/\"):\n",
    "    print(\"nome_do_arquivo:\", arquivo[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceb380c4-4711-4974-a185-966ef9f3998a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Chamada da biblioteca do Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd558918-c2a5-4990-ba6a-91a3b95a8d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93fecfbf-19ab-45b2-9061-08e7dfeeb477",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cria uma função para extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3d8688-8e18-4c2e-89e3-a2fd9e83399a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(dir_leitura, columns_name, index_col_name):\n",
    "\n",
    "    df_res = ps.DataFrame() #inicializa um dataframe em branco\n",
    "    #percorre todos os arquivos no diretorio de arquivos brutos\n",
    "    for arquivo in dbutils.fs.ls(dir_leitura):\n",
    "        print(\"nome_do_arquivo:\", arquivo[1])\n",
    "        dir_leitura_arquivo = dir_leitura + arquivo[1]\n",
    "        #faz a leitura do arquivo corrente\n",
    "        df = ps.read_parquet(dir_leitura_arquivo, columns=columns_name, index_col=index_col_name)\n",
    "        #concatena o resultado corrente com o dataframe final\n",
    "        df_res = ps.concat([df_res, df], ignore_index=True)\n",
    "    \n",
    "    print(\"resultado extraido com sucesso\")\n",
    "\n",
    "    # retorna o resultado final extraido\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f12d38-0c32-4cc1-bab5-7fb216081dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criar uma forma de identificar se existe arquivos em diretório do DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c021c792-b842-4155-ae5f-110a7fef3c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Quando não existe nenhum arquivo com o diretório passado, é gerado o seguinte erro: \"java.io.FileNotFoundException\"\n",
    "dbutils.fs.ls(\"/FileStore/tables/dado_carregado/dado_consolidado.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d610c3f6-8807-485c-839e-5b8ba4ec9c8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir_escrita = \"/FileStore/tables/dado_carregado/dado_consolidado.parquet\"\n",
    "#Com uma exceção, é possível efetuar o processamento correto quando um diretório ainda não existe\n",
    "try:\n",
    "    dbutils.fs.ls(dir_escrita)\n",
    "except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "        print(\"Arquivo não encontrado, primeiro processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b465d49-27f6-4ed6-bb3b-003858d0d7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cria uma função para carregar os dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4eb76c-c6b0-4abc-9dbc-b11290c727d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df_new, dir_escrita_bruto):\n",
    "    try: #caso já exista o arquivo transformado, segue direto com a concatenação e com a carga do resultado final\n",
    "        arquivo = dbutils.fs.ls(dir_escrita_bruto)\n",
    "        df_res = ps.read_parquet(dir_escrita_bruto)\n",
    "        df_res = ps.concat([df_res, df_new])\n",
    "        df_res.to_parquet(dir_escrita_bruto)\n",
    "\n",
    "    except Exception as e: #caso o arquivo transformando ainda não exista, quer dizer que é o primeiro processo do pipeline e é preciso criar o arquivo destino\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Arquivo não encontrado, primeiro processamento\")\n",
    "            df_new.to_parquet(dir_escrita_bruto)\n",
    "\n",
    "    print(\"resultado carregado com sucesso\")\n",
    "\n",
    "    #realiza a movimentação da pasta de dados brutos para a pasta de dados brutos já carregados\n",
    "    dbutils.fs.mv(\"/FileStore/tables/\", \"/FileStore/tables/dados_brutos_carregados/\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35411ae1-7c69-4424-ae43-5bbd4ae151f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cria uma função para a transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a8bf8b-9df4-48fd-9932-392e3abd6a38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(dir_escrita_bruto, dir_escrita_transformado):\n",
    "\n",
    "    df = ps.read_parquet(dir_escrita_bruto) \n",
    "    #Coluna de total de atraso que é a soma do atraso na chegada no local de origem e destino\n",
    "    df[\"Total_Delay\"] = df[\"ArrDelayMinutes\"] + df[\"DepDelayMinutes\"]\n",
    "\n",
    "    #Filtra o estado de origem igual a Texas e o estado destino igual a California\n",
    "    df = df[(df[\"OriginStateName\"] == \"Texas\") | (df[\"DestStateName\"] == \"California\")]\n",
    "\n",
    "    #Agrupa os resultados pelo nome da cidade de origem e calcula a média do total de atrasos e o total da distância percorrida.\n",
    "    df = df.groupby([\"Year\",\"Month\"]).agg(Total_Delay_Average=('Total_Delay', 'mean'), Total_Distance=('Distance', 'sum'))\n",
    "\n",
    "    #reseta o index\n",
    "    df = df.reset_index()\n",
    "\n",
    "    #Armazena o resultado transformado\n",
    "    df.to_parquet(dir_escrita_transformado)\n",
    "\n",
    "    print(\"transformação realizada com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186ad0c0-9208-4ccb-8636-efa776cb1167",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### cria uma função para o ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b002d2d1-8b81-4eaa-b422-31002dcc3b19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def elt(columns_name, index_col_name, dir_leitura, dir_escrita_bruto, dir_escrita_transformado):\n",
    "    print(\"inicializa o ELT\")\n",
    "    try: # verifica se existe novos dados para serem processados\n",
    "        arquivo = dbutils.fs.ls(dir_leitura)\n",
    "        print(\"Arquivos novos encontrados\")\n",
    "        #faz a chamada da extração, da carga e da transformação dos dados\n",
    "        df_new = extract_data(dir_leitura, columns_name, index_col_name)\n",
    "        load_data(df_new, dir_escrita_bruto)\n",
    "        transform_data(dir_escrita_bruto, dir_escrita_transformado)\n",
    "    except Exception as e: #caso não exista nenhum dado novo, retorna com a mensagem e encerra o processo\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Nenhum dado novo\")\n",
    "        else:\n",
    "            print(\"erro no ELT:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd3b578-8c27-47a8-8520-5ebdddd3f607",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### faz a chamada do ELT para um teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d1cef40-f92f-4eee-8bea-591fdff10b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#define o nome dados colunas do arquivo brutos que serão carregadas\n",
    "columns_name = [\"FlightDate\",\n",
    "\"Year\",\n",
    "\"Month\",\n",
    "\"Airline\", \n",
    "\"Cancelled\", \n",
    "\"Diverted\", \n",
    "\"CRSDepTime\", \n",
    "\"DepTime\", \n",
    "\"DepDelayMinutes\",\n",
    "\"DepDelay\", \n",
    "\"ArrTime\", \n",
    "\"ArrDelayMinutes\",\n",
    "\"AirTime\", \n",
    "\"CRSElapsedTime\",\n",
    "\"ActualElapsedTime\",\n",
    "\"Distance\", \n",
    "\"OriginCityName\",\n",
    "\"OriginStateName\",\n",
    "\"DestCityName\", \n",
    "\"DestStateName\"]\n",
    "\n",
    "#define o indice do arquivo bruto que será carregado\n",
    "index_col_name = [\"Flight_Number_Operating_Airline\"]\n",
    "\n",
    "#define o diretório dos arquivos brutos\n",
    "dir_leitura = \"/FileStore/tables/\"\n",
    "#define o diretório de qual será o arquivo parquet final que irá armazenar os dados carregados\n",
    "dir_escrita_bruto = \"/FileStore/tables/dado_carregado/dado_consolidado.parquet\"\n",
    "#define o diretório de qual será o arquivo parquet final que irá armazenar os dados transformados\n",
    "dir_escrita_transformado = \"/FileStore/tables/dados_transformados/dado_transformado.parquet\"\n",
    "\n",
    "#faz a chmada do ELT\n",
    "elt(columns_name, index_col_name, dir_leitura, dir_escrita_bruto, dir_escrita_transformado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf534b8d-01a5-4036-b914-3f1721d9434a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imprime o resultado do primeiro processamento de dados com os arquivos dos anos de 2020 e 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809e8317-4545-4c02-8f1d-14bc2740832e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = ps.read_parquet(dir_escrita_transformado)\n",
    "df = df.sort_values(by=[\"Year\", \"Month\"])\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca106926-ef2d-4015-adb1-50411a9822c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### verifica se os dados processados foram movimentados para a pasta de dados brutos carregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ec31a0-4b24-4844-bc88-aae235723bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/dados_brutos_carregados/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec8b72a5-cd76-48ab-8831-bd82f559b111",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cria uma função para contar um tempo e permitir um processamento a cada periodo de tempo definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad36684-c105-4fc5-be0c-e559c2b08ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time #utiliza a função time do python\n",
    "\n",
    "tempo_proc = 5 #segundos\n",
    "\n",
    "print(\"inicializa contagem\")\n",
    "for t in range(1, tempo_proc+1):\n",
    "    print(t)\n",
    "    time.sleep(1) #utiliza a função time sleep para forçar aguardar 1 segundo a cada iteração no loop\n",
    "print(\"finaliza contagem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecd85bd-6663-408e-a025-1dd75babb62c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# função para chamar a contagem de \"tempo_proc\" segundos\n",
    "def timer(tempo_proc):\n",
    "    print(\"inicializa contagem\")\n",
    "    for t in range(1, tempo_proc+1):\n",
    "        print(t)\n",
    "        time.sleep(1)\n",
    "    print(\"finaliza contagem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9769b207-ad27-4034-87c7-90af18cb8729",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Definição dos parametros para a chamada definitiva do processamento em lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d8e5fa-e493-4def-adaa-c97c78960bb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#define o nome dados colunas do arquivo brutos que serão carregadas\n",
    "columns_name = [\"FlightDate\",\n",
    "\"Year\",\n",
    "\"Month\",\n",
    "\"Airline\", \n",
    "\"Cancelled\", \n",
    "\"Diverted\", \n",
    "\"CRSDepTime\", \n",
    "\"DepTime\", \n",
    "\"DepDelayMinutes\",\n",
    "\"DepDelay\", \n",
    "\"ArrTime\", \n",
    "\"ArrDelayMinutes\",\n",
    "\"AirTime\", \n",
    "\"CRSElapsedTime\",\n",
    "\"ActualElapsedTime\",\n",
    "\"Distance\", \n",
    "\"OriginCityName\",\n",
    "\"OriginStateName\",\n",
    "\"DestCityName\", \n",
    "\"DestStateName\"]\n",
    "\n",
    "#define o indice do arquivo bruto que será carregado\n",
    "index_col_name = [\"Flight_Number_Operating_Airline\"]\n",
    "\n",
    "#define o diretório dos arquivos brutos\n",
    "dir_leitura = \"/FileStore/tables/\"\n",
    "#define o diretório de qual será o arquivo parquet final que irá armazenar os dados carregados\n",
    "dir_escrita_bruto = \"/FileStore/tables/dado_carregado/dado_consolidado.parquet\"\n",
    "#define o diretório de qual será o arquivo parquet final que irá armazenar os dados transformados\n",
    "dir_escrita_transformado = \"/FileStore/tables/dados_transformados/dado_transformado.parquet\"\n",
    "\n",
    "#definição do parametro do contador de 5 segundos\n",
    "tempo_proc = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c041756-043f-40e5-8d37-9f0d99158c52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Chamada definitiva do processamento em lotes a cada 5 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599d03be-0853-4bfc-897b-505a08bd3549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#loop infinitivo chamando o contador e o ELT \n",
    "while 1==1:\n",
    "    timer(tempo_proc)\n",
    "    elt(columns_name, index_col_name, dir_leitura, dir_escrita_bruto, dir_escrita_transformado)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aula_3_Parte_1_Cargas_em_Batches",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
